{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ddcf404",
   "metadata": {},
   "source": [
    "# Quiz 2 Preparation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0694d02d",
   "metadata": {},
   "source": [
    "## 1 Principal Component Analysis (PCA)\n",
    "#### 1.1 What is PCA?\n",
    "\n",
    "**Definition** :Principal Component Analysis is a dimensionality reduction technique that transforms correlated variables into a set of uncorrelated variables called principal components (PCs). The key goals are:\n",
    "\n",
    "1. Reduce dimensionality while retaining maximum variance\n",
    "\n",
    "2. Create uncorrelated features from correlated ones\n",
    "\n",
    "3. Identify patterns in high-dimensional data\n",
    "\n",
    "#### 1.2 Standardization: When and Why\n",
    "\n",
    "**CRITICAL CONCEPT:** Before running PCA, you must understand whether to standardize (center and scale) your data.\n",
    "\n",
    "**1.2.1 Centering**\n",
    "Centering means subtracting the mean from each variable:\n",
    "\n",
    "$x_{centered} = x − \\bar{x}$ (1)\n",
    "\n",
    "**1.2.2 Scaling**\n",
    "\n",
    "Scaling means dividing by the standard deviation after centering:\n",
    "\n",
    "$x_{scaled} = \\frac{x_{centered}}{s_x}$ (2)\n",
    "\n",
    "When to standardize:\n",
    "\n",
    "1. Variables are measured in different units (e.g., weight in kg, height in cm)\n",
    "2. Variables have vastly different variances\n",
    "3. You want each variable to contribute equally to the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "30dc469c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance ratios: [0.96296464 0.03703536]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "X = [[2.5, 2.4],\n",
    "     [0.5, 0.7],\n",
    "     [2.2, 2.9],\n",
    "     [1.9, 2.2],\n",
    "     [3.1, 3.0],\n",
    "     [2.3, 2.7],\n",
    "     [2, 1.6],\n",
    "     [1, 1.1],\n",
    "     [1.5, 1.6],\n",
    "     [1.1, 0.9]]\n",
    "\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler () # This centers AND scales\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Then run PCA\n",
    "pca = PCA()\n",
    "pca.fit(X_scaled)\n",
    "print(\"Explained variance ratios:\", pca.explained_variance_ratio_) #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a5c66deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained Variance Ratios: [0.96296464 0.03703536]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "X = [[2.5, 2.4],\n",
    "     [0.5, 0.7],\n",
    "     [2.2, 2.9],\n",
    "     [1.9, 2.2],\n",
    "     [3.1, 3.0],\n",
    "     [2.3, 2.7],\n",
    "     [2, 1.6],\n",
    "     [1, 1.1],\n",
    "     [1.5, 1.6],\n",
    "     [1.1, 0.9]]\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler () # This centers and scales the data\n",
    "X_scaled = scaler.fit_transform(X) \n",
    "\n",
    "# Then Run PCA\n",
    "pca = PCA()\n",
    "pca.fit(X_scaled)\n",
    "print(\"Explained Variance Ratios:\", pca.explained_variance_ratio_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34783725",
   "metadata": {},
   "source": [
    "#### 1.3 Interpreting PCA Results\n",
    "\n",
    "**1.3.1 Principal Component Loadings**\n",
    "\n",
    "The loadings (also called rotation matrix or components) tell you how each original variable\n",
    "contributes to each PC."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80af2d9c",
   "metadata": {},
   "source": [
    "**How to read loadings:**\n",
    "\n",
    "1. Each column represents one principal component\n",
    "\n",
    "2. Each row shows how much that original variable contributes\n",
    "\n",
    "3. Positive values: variable increases with PC\n",
    "\n",
    "4. Negative values: variable decreases with PC\n",
    "\n",
    "5. Larger absolute values = stronger contribution\n",
    "\n",
    "\n",
    " **Get the loadings:**\n",
    "loadings = pca.components_.T  # Transpose to get variables x PCs\n",
    "\n",
    "print(loadings)\n",
    "\n",
    " Variable | PC1 | PC2 | PC3 | PC4 |\n",
    "| :------- | :------: | -------: | :------: | :-------: |\n",
    "| Var1    | 0.50   | -0.50    | 0.20   | -0.70 |\n",
    "| Var2   | 0.49  | 0.51   | -0.30  | 0.10  |\n",
    "| Var3    | 0.51   | 0.48   | 0.40   | 0.20  |\n",
    "| Var4   | 0.50   | -0.49  | -0.30  | 0.67  |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ed0824",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "**1.3.2 Computing PC Scores**\n",
    "\n",
    "EXAM TIP: You must know how to write the formula for PC scores!\n",
    "\n",
    "If data is NOT standardized, PC1 score for observation i is:\n",
    "\n",
    "$PC1_i = w1 ×x1_i + w2 ×x2_i + ... + wp ×xp_i$ (3)\n",
    "\n",
    "\n",
    "If data IS standardized, PC1 score for observation i is:\n",
    "\n",
    "$PC1_i = w_1 × \\frac{(x1_i − \\bar{x_1})}{s_1} + w_2 × \\frac{(x2_i − \\bar{x_2})}{s_2} + ... + w_p × \\frac{(xp_i − \\bar{x_p})}{s_p}$ (4)\n",
    "\n",
    "where w_j are the loadings for PC1,  ̄x_j are means, and s_j are standard deviations.\n",
    "Example: Given loadings [0.5, 0.5, 0.5, 0.5] and standardized data, PC1 equals:\n",
    "\n",
    "$PC1 = 0.5 × \\frac{(x_1 − \\bar{x_1})}{s_1} + 0.5 × \\frac{(x_2 − \\bar{x_2})}{s_2} + 0.5 × \\frac{(x_3 − \\bar{x_3})}{s_3} + 0.5 × \\frac{(x_4 − \\bar{x_4})}{s_4}$ (5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e4a862",
   "metadata": {},
   "source": [
    "#### 1.4 Variance and PCA\n",
    "\n",
    "**KEY FACT: Principal components are ordered by variance!**\n",
    "\n",
    "•PC1 has the largest variance of all possible linear combinations\n",
    "\n",
    "•PC2 has the second largest variance, uncorrelated with PC1\n",
    "\n",
    "•PC3 has the third largest variance, uncorrelated with PC1 and PC2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c6e237c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.13992141 0.08230081]\n",
      "Total Variance explained: 2.2222222222222228\n"
     ]
    }
   ],
   "source": [
    "# Get the variance explained by each PC\n",
    "import numpy as np\n",
    "var_explained = pca.explained_variance_\n",
    "print(var_explained)\n",
    "\n",
    "#Total Variance\n",
    "total_variance = np.sum(var_explained)\n",
    "print(\"Total Variance explained:\", total_variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df372fbe",
   "metadata": {},
   "source": [
    "##### 1.5 Proportion of Variance Explained (PVE)\n",
    "\n",
    "PVE tells you what percentage of total variance each PC captures.\n",
    "\n",
    "$PVE_j = \\frac{Variance of PCj}{Total Variance} = \\frac{Variance of PCj}{\\sum_{i=1}^{p} Variance of PCi}$\n",
    "\n",
    "Total Variance = $\\sum_{i=1}^{p} \\text{Variance of PCi}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0ec65e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.96296464 0.03703536]\n",
      "0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "##### Calculate PVE\n",
    "\n",
    "pve = pca.explained_variance_ratio_\n",
    "print(pve)\n",
    "\n",
    "[0.80 , 0.125 , 0.05, 0.025]\n",
    "\n",
    "6 # This means:\n",
    "\n",
    "7 # PC1 explains 80% of total variance\n",
    "\n",
    "8 # PC2 explains 12.5% of total variance\n",
    "\n",
    "9 # PC3 explains 5% of total variance\n",
    "\n",
    "10 # PC4 explains 2.5% of total variance\n",
    "\n",
    "12 # They sum to 1 (100%)\n",
    "\n",
    "print(np.sum(pve)) # 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c4cfa7",
   "metadata": {},
   "source": [
    "##### 1.6 Practice Problems: PCA\n",
    "\n",
    "Problem 1: You run PCA on 5 variables after centering and scaling. The loadings for PC1 are\n",
    "[0.45, 0.45, 0.44, 0.46, 0.45]. Write the formula for computing PC1 scores. Include the means and\n",
    "standard deviations in your formula.\n",
    "\n",
    "Problem 2: Given PVE values [0.65, 0.20, 0.10, 0.05], what percentage of variance does PC1\n",
    "capture? What about PC1 and PC2 combined?\n",
    "\n",
    "Problem 3: True or False: If you run PCA on standardized data and all variables have\n",
    "equal weight (similar loadings) in PC1, then PC1 is approximately an average of the standardized\n",
    "variables.\n",
    "\n",
    "Problem 4: You have 4 PCs with variances [16, 3, 0.8, 0.2]. Calculate the PVE for each PC.\n",
    "Which PC explains the most variance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e34330c",
   "metadata": {},
   "source": [
    "## 2. K-means clustering:\n",
    "\n",
    "##### 2.1 What is K-Means?\n",
    "K-means is an unsupervised learning algorithm that partitions data into k clusters. Each observation belongs to the cluster with the nearest mean (centroid).\n",
    "\n",
    "##### 2.2 How K-Means Works\n",
    "1. Choose number of clusters k\n",
    "2. Randomly initialize k cluster centroids\n",
    "3. Assign each point to nearest centroid\n",
    "4. Recalculate centroids based on assigned points\n",
    "5. Repeat steps 3-4 until convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a8e52ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 0 0 0 1 1 1]\n",
      "[6 4]\n",
      "[[2.33333333 2.46666667]\n",
      " [1.025      1.075     ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Fit k-means with 2 clusters\n",
    "kmeans = KMeans(n_clusters =2, random_state =42)\n",
    "kmeans.fit(X)\n",
    "\n",
    "# Get cluster assignments (0 or 1 for 2 clusters)\n",
    "labels = kmeans.labels_\n",
    "print(labels) # [0, 1, 0, 1, 1, 0, ...]\n",
    "\n",
    "# Count observations in each cluster\n",
    "unique , counts = np.unique(labels , return_counts=True)\n",
    "print(counts) # [6, 4] means 6 in cluster 0, 4 in cluster 1\n",
    "\n",
    "# Get cluster centers\n",
    "centers = kmeans.cluster_centers_\n",
    "print(centers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd9db1b",
   "metadata": {},
   "source": [
    "##### 2.4 Important Notes\n",
    "\n",
    "EXAM TIP: Pay attention to:\n",
    "\n",
    "•The random state parameter - affects initialization\n",
    "\n",
    "•Cluster labels are arbitrary (0, 1, 2, ...) - no inherent ordering\n",
    "\n",
    "•Cluster sizes are NOT necessarily equal\n",
    "\n",
    "•Results depend on initialization and can vary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52f0e1c",
   "metadata": {},
   "source": [
    "##### 2.5 Interpreting cluster sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ac8592c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6 4]\n"
     ]
    }
   ],
   "source": [
    "# If I get an output like:\n",
    "unique, counts = np.unique(labels, return_counts = True)\n",
    "print(counts)\n",
    "# We get [6, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61810f10",
   "metadata": {},
   "source": [
    "This means:\n",
    "- There are two clusters (length of array)\n",
    "- Cluster 0 has 6 observations\n",
    "- Cluster 1 has 4 observations\n",
    "Total: 10 observations\n",
    "\n",
    "**Common Question Format: ”There are X subjects in cluster 1 and Y in cluster 2”**\n",
    "\n",
    "- Make sure X + Y = total number of observations\n",
    "- Check which number corresponds to which cluster\n",
    "- Labels can be 0-indexed or 1-indexed depending on context\n",
    "\n",
    "##### 2.6 Practice Problems: Clustering\n",
    "\n",
    "**Problem 1:** You run k-means with k=3 on 300 observations. The output shows counts [100, 125,\n",
    "75]. How many observations are in each cluster? Do clusters have equal sizes?\n",
    "- 100, 125, 75 observations\n",
    "- clusters do not have same sizes\n",
    "\n",
    "**Problem 2:** True or False: K-means always produces clusters of equal size.\n",
    "- False\n",
    "\n",
    "**Problem 3:** If you run k-means twice with different random seeds, will you get the same cluster\n",
    "sizes? Why or why not?\n",
    "- False because different seeds change initialization points which can lead to changes in local optima and different cluster sizes \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7365bb3",
   "metadata": {},
   "source": [
    "## 3 Linear Regression\n",
    "##### 3.1 Simple Linear Regression\n",
    "Simple linear regression models the relationship between one predictor X and response Y :\n",
    "\n",
    "Y = β0 + β1X + ε\n",
    "\n",
    "where:\n",
    "\n",
    "- β0 = intercept\n",
    "- β1 = slope (coefficient)\n",
    "- ε = random error\n",
    "\n",
    "##### 3.2 Interpreting Coefficients\n",
    "\n",
    "**The Slope (β1):** ”On average, for every 1 unit increase in X, Y changes by β1 units.”\n",
    "\n",
    "CRITICAL DISTINCTION:\n",
    "- Average effect: On average, Y decreases by β1 when X increases by 1\n",
    "- Individual predictions: For specific observations, actual values can vary around the pre-\n",
    "dicted mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cde95b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output:\n",
    "# Intercept: 35.82\n",
    "# Coefficient: -0.044\n",
    "# This means:\n",
    "# Predicted MPG_Hwy = 35.82 - 0.044 * Horsepower"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ef41d5",
   "metadata": {},
   "source": [
    "Interpretation:\n",
    "\n",
    "- On average, MPG Hwy decreases by 0.044 for each 1 unit increase in Horsepower\n",
    "- A car with Horsepower=200 has predicted MPG = 35.82 - 0.044(200) = 27.02\n",
    "- A car with Horsepower=201 has predicted MPG = 35.82 - 0.044(201) = 26.976\n",
    "\n",
    "**EXAM TIP:** Watch out for tricky questions!\n",
    "\n",
    "***TRUE Statement:*** ”On average, MPG decreases by 0.044 when Horsepower increases by 1.”\n",
    "\n",
    "***FALSE Statement:*** ”For any two specific cars where one has Horsepower=200 and another has Horsepower=201, the first car is GUARANTEED to have higher MPG.”\n",
    "\n",
    "Why is the second false? Because individual observations have variability around the regression\n",
    "line. The model predicts the mean, not individual values exactly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c058f4",
   "metadata": {},
   "source": [
    "##### 3.3 Multiple Linear Regression\n",
    "\n",
    "Multiple regression includes multiple predictors:\n",
    "\n",
    "\n",
    "Y = β0 + β1X1 + β2X2 + ... + βpXp + ε \n",
    "\n",
    "**CRITICAL CONCEPT:** Interpretation of coefficients in multiple regression\n",
    "\n",
    "β1 = ”The average change in Y for a 1-unit increase in X_1, ***holding all other variables\n",
    "constant.”***\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dbff77ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output:\n",
    "# Intercept: 46.99\n",
    "# Horsepower: -0.028\n",
    "# Weight: -4.01\n",
    "\n",
    "# Model: MPG_Hwy = 46.99 - 0.028* Horsepower - 4.01* Weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9667e3",
   "metadata": {},
   "source": [
    "How to interpret -0.028 for Horsepower:\n",
    "\n",
    "- ”On average, for each 1 unit increase in Horsepower, MPG Hwy decreases by 0.028, when\n",
    "Weight is held constant.”\n",
    "\n",
    "- You cannot say ”1 unit increase in Horsepower always decreases MPG by 0.028” without the\n",
    "”holding Weight constant” qualifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c958ce57",
   "metadata": {},
   "source": [
    "##### 3.4 Making Predictions\n",
    "\n",
    "To predict: Plug values into the equation\n",
    "\n",
    " **IMPORTANT: You can only use variables that are in the model!**\n",
    "- If Seats and Length are not in the model, you don’t need their values to predict\n",
    "- Having extra information doesn’t hurt - just ignore variables not in the model\n",
    "- You cannot make predictions if required variables are missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2bb50dd6",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['Horsepower'], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Simple regression: Y ~ C1\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m c \u001b[38;5;241m=\u001b[39m car_data[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHorsepower\u001b[39m\u001b[38;5;124m'\u001b[39m]] \u001b[38;5;66;03m# Need double brackets for DataFrame\u001b[39;00m\n\u001b[1;32m     14\u001b[0m y \u001b[38;5;241m=\u001b[39m car_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMPG_Hwy\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     16\u001b[0m model \u001b[38;5;241m=\u001b[39m LinearRegression ()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py:3899\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3897\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   3898\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 3899\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39m_get_indexer_strict(key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   3901\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   3902\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:6115\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6112\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6113\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6115\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[1;32m   6117\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   6118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6119\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:6176\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6174\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_interval_msg:\n\u001b[1;32m   6175\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 6176\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6178\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m   6179\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of [Index(['Horsepower'], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "# Given: MPG_Hwy = 46.99 - 0.028* Horsepower - 4.01* Weight\n",
    "# Predict for: Horsepower =240, Weight =3.5\n",
    "\n",
    "predicted_MPG = 46.99 - 0.028*240 - 4.01*3.5\n",
    "predicted_MPG = 46.99 - 6.72 - 14.035\n",
    "predicted_MPG = 26.235\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "\n",
    "# Simple regression: Y ~ C1\n",
    "c = car_data[['Horsepower']] # Need double brackets for DataFrame\n",
    "y = car_data['MPG_Hwy']\n",
    "\n",
    "model = LinearRegression ()\n",
    "model.fit(X, y)\n",
    "print(f\"Intercept:␣{model.intercept_}\")\n",
    "print(f\"Coefficient:␣{model.coef_ [0]}\")\n",
    "\n",
    "# Multiple regression: Y ~ C1 + C2\n",
    "c_multi = car_data [['Horsepower', 'Weight']]\n",
    "y = car_data['MPG_Hwy']\n",
    "model2 = LinearRegression ()\n",
    "model2.fit(c_multi , y)\n",
    "\n",
    "print(f\"Intercept:␣{model2.intercept_}\")\n",
    "print(f\"Coefficients:␣{model2.coef_}\")\n",
    "# model2.coef_ [0] is coefficient for Horsepower\n",
    "# model2.coef_ [1] is coefficient for Weight\n",
    "\n",
    "# Make predictions\n",
    "new_data = np.array ([[240 , 3.5]]) # Horsepower =240, Weight =3.5\n",
    "prediction = model2.predict(new_data)\n",
    "print(f\"Predicted␣MPG:␣{prediction [0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e70faa",
   "metadata": {},
   "source": [
    "##### 3.6 Model Quality: R²\n",
    "\n",
    "R² (R-squared) measures the proportion of variance in Y explained by the model.\n",
    "\n",
    "$$\n",
    "R^2 \\;=\\; 1 - \\frac{SS_{\\text{residual}}}{SS_{\\text{total}}}\n",
    "\\;=\\;\n",
    "1 - \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}\n",
    "$$\n",
    "\n",
    "Interpretation:\n",
    "- R² = 0.465 means “The model explains 46.5% of the variance in Y.”\n",
    "- Range: 0 to 1 (0% to 100%).\n",
    "- Higher R² = better fit.\n",
    "- R² always increases (or stays the same) when adding more variables.\n",
    "\n",
    "Adjusted R² (penalizes unhelpful variables):\n",
    "$$\n",
    "R^2_{\\text{adj}} \\;=\\; 1 - (1 - R^2)\\,\\frac{n - 1}{n - p - 1}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- n = number of observations\n",
    "- p = number of predictors (excluding the intercept)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0f275f",
   "metadata": {},
   "source": [
    "##### 3.7 F-Test for Overall Model Significance\n",
    "\n",
    "The F-test checks whether at least one predictor is useful.\n",
    "\n",
    "Null and alternative hypotheses:\n",
    "$$\n",
    "H_0:\\ \\beta_1 = \\beta_2 = \\cdots = \\beta_p = 0\n",
    "\\qquad\n",
    "H_A:\\ \\text{at least one } \\beta_j \\neq 0\n",
    "$$\n",
    "\n",
    "F-statistic:\n",
    "$$\n",
    "F \\;=\\; \\frac{R^2/p}{(1 - R^2)/(n - p - 1)}\n",
    "$$\n",
    "where:\n",
    "- $p$ = number of predictors (excluding intercept)\n",
    "- $n$ = sample size\n",
    "\n",
    "Exam tip: Relationship between F-test and $R^2$\n",
    "- Larger $R^2$ generally leads to a larger $F$-statistic.\n",
    "- You still need the F-test to assess if $R^2$ is “large enough” to be statistically significant.\n",
    "- Large $R^2$ alone does not prove significance; use the F-test p-value.\n",
    "\n",
    "How to reject $H_0$:\n",
    "- If p-value < $\\alpha$ (significance level), reject $H_0$.\n",
    "- Example: p-value = 0.0001, $\\alpha$ = 0.001 → 0.0001 < 0.001 → reject $H_0$.\n",
    "- “p-value < $2\\times 10^{-16}$” means the p-value is extremely small (essentially 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3566913f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mstats\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Calculate F-statistic\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(y)\n\u001b[1;32m      4\u001b[0m p \u001b[38;5;241m=\u001b[39m X_multi\u001b[38;5;241m.\u001b[39mshape [\u001b[38;5;241m1\u001b[39m] \u001b[38;5;66;03m# number of predictors\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Get R-squared\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y' is not defined"
     ]
    }
   ],
   "source": [
    "import scipy.stats as stats\n",
    "# Calculate F-statistic\n",
    "n = len(y)\n",
    "p = X_multi.shape [1] # number of predictors\n",
    "\n",
    "# Get R-squared\n",
    "y_pred = model2.predict(X_multi)\n",
    "r2 = r2_score(y, y_pred)\n",
    "\n",
    "# Calculate F-statistic\n",
    "f_stat = (r2 / p) / ((1 - r2) / (n - p - 1))\n",
    "\n",
    "# Get p-value\n",
    "f_pvalue = 1 - stats.f.cdf(f_stat , p, n - p - 1)\n",
    "\n",
    "print(f\"F-statistic:␣{f_stat}\")\n",
    "print(f\"p-value:␣{f_pvalue}\")\n",
    "\n",
    "# Interpretation:\n",
    "if f_pvalue < 0.001:\n",
    "    print(\"Reject H0 at alpha =0.001\")\n",
    "\n",
    "print(\"At least one predictor is significant\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fe49f8",
   "metadata": {},
   "source": [
    "###### 3.8 Practice Problems: Regression\n",
    "\n",
    "**Problem 1: Given the model: ˆY = 50 −0.05X**\n",
    "\n",
    "- What is the predicted Y when X=100?\n",
    "    - Answer: 45\n",
    "- What is the predicted Y when X=101?\n",
    "    - Answer: 44.95\n",
    "- On average, what happens to Y when X increases by 1?\n",
    "    - Answer: For every 1 unit X increases by Y decreases by 0.05. \n",
    "- If person A has X=100 and person B has X=101, is Y guaranteed to be higher for person A?\n",
    "    - Answer: False, expected value yes, but regression calculates the mean. There is variability for individual points around the mean.\n",
    "\n",
    "**Problem 2: Given: ˆY = 30 + 2X1 −5X2**\n",
    "\n",
    "- Interpret the coefficient of X1\n",
    "    - Answer: Holding other variables constant, for every 1 unit X1 increases, Y increases by 2\n",
    "- Interpret the coefficient of X2\n",
    "    - Answer: Holding other variables constant, for every 1 unit X2 increases, Y decreases by a factor of 5\n",
    "- Predict Y when X1 = 10,X2 = 3 \n",
    "    - Answer: 50-15 = 35\n",
    "- Can you predict Y if you’re given X1 = 10,X2 = 3,X3 = 5?\n",
    "    - Answer: Yes! Y would still be 35, but X3 isn't factored into the regression. Extra information is nice, but unncessary in this case.\n",
    "\n",
    "**Problem 3: A model has R2 = 0.65 and p=2 predictors, n=200 observations. Calculate the F-statistic.**\n",
    "- Answer: around 183\n",
    "\n",
    "**Problem 4: True or False: ”If R2 = 0.8, we can automatically reject H0 : β1 = β2 = 0 at α = 0.001.” Explain.**\n",
    "- Answer: False, need F test to determine whether H0 should be rejected\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6cdfea",
   "metadata": {},
   "source": [
    "## 4 Model Diagnostics\n",
    "##### 4.1 Residual Plots\n",
    "\n",
    "Residuals are the differences between observed and predicted values:\n",
    "\n",
    "$$\n",
    "e_i = yi − \\hat{y_i} \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c12757d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#4.1.1 Residuals vs. Fitted Values Plot\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# This plot helps check the linearity assumption.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Create residual plot\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X)\n\u001b[1;32m      6\u001b[0m residuals \u001b[38;5;241m=\u001b[39m y \u001b[38;5;241m-\u001b[39m y_pred\n\u001b[1;32m      8\u001b[0m plt\u001b[38;5;241m.\u001b[39mscatter(y_pred , residuals)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "#4.1.1 Residuals vs. Fitted Values Plot\n",
    "# This plot helps check the linearity assumption.\n",
    "\n",
    "# Create residual plot\n",
    "y_pred = model.predict(X)\n",
    "residuals = y - y_pred\n",
    "\n",
    "plt.scatter(y_pred , residuals)\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.xlabel('Fitted values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residuals vs Fitted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02800537",
   "metadata": {},
   "source": [
    "**What to look for:**\n",
    "\n",
    "- ***Good:*** Random scatter around horizontal line at 0\n",
    "\n",
    "- ***Bad:*** Clear patterns, curves, or systematic deviations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5f14c9",
   "metadata": {},
   "source": [
    "##### 4.2 Interpreting Residual Patterns\n",
    "\n",
    "Pattern 1: Underestimation\n",
    "- If residuals are mostly positive (above 0) for certain fitted values\n",
    "- This means $ yi − \\hat{y_i}$  > 0, so $y_i > \\hat{y_i}$\n",
    "- The actual values are higher than predictions\n",
    "- We are underestimating (predicting too low)\n",
    "\n",
    "Pattern 2: Overestimation\n",
    "\n",
    "- If residuals are mostly negative (below 0) for certain fitted values\n",
    "- This means $ yi − \\hat{y_i}$  < 0, so $y_i < \\hat{y_i}$\n",
    "- The actual values are lower than predictions\n",
    "- We are overestimating (predicting too high)\n",
    "\n",
    "**EXAM TIP: Common question pattern**\n",
    "\n",
    "”For cars with smaller MPG Hwy (left side of plot), residuals are positive. This suggests:”\n",
    "- Positive residuals = actual > predicted\n",
    "- We are predicting too low\n",
    "- We are underestimating\n",
    "- Linearity might be a problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41a959c",
   "metadata": {},
   "source": [
    "##### 4.4 Practice Problems: Diagnostics\n",
    "\n",
    "**Problem 1:** You fit a model predicting house prices. The residual plot shows that for expensive\n",
    "houses (right side, high fitted values), most residuals are negative. Are you overestimating or\n",
    "underestimating expensive houses?\n",
    "\n",
    "- Answer: negative residuals mean you are overestimating  \n",
    "\n",
    "**Problem 2:** For a model predicting test scores, you observe positive residuals for students with\n",
    "low predicted scores. What does this suggest about the model’s predictions for low-performing\n",
    "students?\n",
    "\n",
    "- Answer: positive residuals mean you are underestimating\n",
    "\n",
    "**Problem 3:** True or False: ”If all residuals on the left side of the plot are above 0, the model\n",
    "is overestimating for small values.”\n",
    "\n",
    "- Answer: False, you are underestimating for small values\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
